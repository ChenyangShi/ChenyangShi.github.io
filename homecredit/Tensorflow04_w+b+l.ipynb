{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "\n",
    "df=pd.read_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['TARGET'].notnull()]\n",
    "test_df = df[df['TARGET'].isnull()]\n",
    "del df\n",
    "gc.collect()\n",
    "feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "\n",
    "X = train_df[feats]\n",
    "y = train_df['TARGET']\n",
    "\n",
    "total = X.isnull().sum().sort_values(ascending = False)\n",
    "percent = (X.isnull().sum() / X.isnull().count()*100).sort_values(ascending = False)\n",
    "missing_X = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "L=missing_X[missing_X['Total']==0]\n",
    "L1=L.reset_index().values\n",
    "nomiss_feats=[]\n",
    "for e in range(0, L.shape[0]):\n",
    "    nomiss_feats.append(L1[e][0])\n",
    "\n",
    "del missing_X\n",
    "del L\n",
    "del L1\n",
    "gc.collect()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for f in X.columns:\n",
    "    if f in nomiss_feats:\n",
    "        X[f] = scaler.fit_transform(X[f].values.reshape(-1,1))\n",
    "        \n",
    "y = scaler.fit_transform(y.values.reshape(-1,1))\n",
    "\n",
    "del nomiss_feats\n",
    "gc.collect()\n",
    "\n",
    "X = X.fillna(value = 0)\n",
    "\n",
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,test_size=0.2)#, random_state=42)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_valid.shape)\n",
    "print (y_train.shape)\n",
    "print (y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and run model\n",
    "import tensorflow as tf\n",
    "ITERATIONS = 40000\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    " \n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Let's train the model\n",
    "feature_count = X_train.shape[1]\n",
    "x = tf.placeholder('float', shape=[None, feature_count], name='x')\n",
    "y_ = tf.placeholder('float', shape=[None, 1], name='y_')\n",
    "\n",
    "print(x.get_shape())\n",
    "\n",
    "nodes = 20\n",
    "\n",
    "w1 = weight_variable([feature_count, nodes])\n",
    "b1 = bias_variable([nodes])\n",
    "l1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "\n",
    "w2 = weight_variable([nodes, 1])\n",
    "b2 = bias_variable([1])\n",
    "y = tf.nn.sigmoid(tf.matmul(l1, w2) + b2)\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(y_*tf.log(tf.maximum(0.00001, y)) + (1.0 - y_)*tf.log(tf.maximum(0.00001, 1.0-y)))\n",
    "reg = 0.01 * (tf.reduce_mean(tf.square(w1)) + tf.reduce_mean(tf.square(w2)))\n",
    "\n",
    "predict = (y > 0.5)\n",
    "\n",
    "correct_prediction = tf.equal(predict, (y_ > 0.5))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "                              \n",
    "                              \n",
    "\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy + reg)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    feed={x:X_train, y_:y_train}\n",
    "    sess.run(train_step, feed_dict=feed)\n",
    "    if i % 1000 == 0 or i == ITERATIONS-1:\n",
    "        print('{} {} {:.2f}%'.format(i, sess.run(cross_entropy, feed_dict=feed), sess.run(accuracy, feed_dict=feed)*100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = test_df[feats]\n",
    "\n",
    "total = T.isnull().sum().sort_values(ascending = False)\n",
    "percent = (T.isnull().sum() / T.isnull().count()*100).sort_values(ascending = False)\n",
    "missing_T = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "L2=missing_T[missing_T['Total']==0]\n",
    "L3=L2.reset_index().values\n",
    "nomiss_feats=[]\n",
    "for e in range(0, L2.shape[0]):\n",
    "    nomiss_feats.append(L3[e][0])\n",
    "    \n",
    "del missing_T\n",
    "del L2\n",
    "del L3\n",
    "gc.collect()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for f in T.columns:\n",
    "    if f in nomiss_feats:\n",
    "        T[f] = scaler.fit_transform(T[f].values.reshape(-1,1))\n",
    "        \n",
    "del nomiss_feats\n",
    "gc.collect()\n",
    "\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
